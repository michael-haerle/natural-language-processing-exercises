{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c361e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "import itertools\n",
    "\n",
    "# To get rid of those blocks of red warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Standard Imports\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn import metrics\n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Vis Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "import sklearn.preprocessing\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# NLP Imports\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Custom Module Imports\n",
    "import env\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96567fb4",
   "metadata": {},
   "source": [
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "- This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8368e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/blog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d4362d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = acquire.get_blog_articles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df151c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diversity Equity and Inclusion Report</td>\n",
       "      <td>Codeup is excited to launch our first Diversit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup Honored as SABJ Diversity and Inclusion...</td>\n",
       "      <td>Codeup has been named the 2022 Diversity and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Can I Finance My Career Transition?</td>\n",
       "      <td>Deciding to transition into a tech career is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tips for Women Beginning a Career in Tech</td>\n",
       "      <td>Codeup strongly values diversity, and inclusio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Cloud Computing and AWS?</td>\n",
       "      <td>With many companies switching to cloud service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022 SABJ C-Suite Award Winner: Stephen Noteboom</td>\n",
       "      <td>Codeup’s Chief Operating Officer, Stephen Note...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              Diversity Equity and Inclusion Report   \n",
       "1  Codeup Honored as SABJ Diversity and Inclusion...   \n",
       "2            How Can I Finance My Career Transition?   \n",
       "3          Tips for Women Beginning a Career in Tech   \n",
       "4                   What is Cloud Computing and AWS?   \n",
       "5   2022 SABJ C-Suite Award Winner: Stephen Noteboom   \n",
       "\n",
       "                                             content  \n",
       "0  Codeup is excited to launch our first Diversit...  \n",
       "1  Codeup has been named the 2022 Diversity and I...  \n",
       "2  Deciding to transition into a tech career is a...  \n",
       "3  Codeup strongly values diversity, and inclusio...  \n",
       "4  With many companies switching to cloud service...  \n",
       "5  Codeup’s Chief Operating Officer, Stephen Note...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc95a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    article = original.lower()\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "    #use re.sub to remove special characters\n",
    "    article = re.sub(r'[^a-z0-9\\'\\s]', '', article)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdbdfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    #create the tokenizer\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    article = tokenize.tokenize(article, return_str=True)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b4f6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    #create porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    #join words back together\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f29ae000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    #join words back together\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "654a5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article):\n",
    "    #save stopwords\n",
    "    stopwords_ls = stopwords.words('english')\n",
    "    words = article.split()\n",
    "    #remove stopwords from list of words\n",
    "    filtered_words = [word for word in words if word not in stopwords_ls]\n",
    "    #join words back together\n",
    "    article = ' '.join(filtered_words)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a72bef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_article(original):\n",
    "#     for x in original.content:\n",
    "#         article = x.lower()\n",
    "#         article = unicodedata.normalize('NFKD', article)\\\n",
    "#         .encode('ascii', 'ignore')\\\n",
    "#         .decode('utf-8')\n",
    "#         #use re.sub to remove special characters\n",
    "#         article = re.sub(r'[^a-z0-9\\'\\s]', '', article)\n",
    "#         #create the tokenizer\n",
    "#         tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "#         #use the tokenizer\n",
    "#         article = tokenize.tokenize(article, return_str=True)\n",
    "#         #save stopwords\n",
    "#         stopwords_ls = stopwords.words('english')\n",
    "#         words = article.split()\n",
    "#         #remove stopwords from list of words\n",
    "#         filtered_words = [word for word in words if word not in stopwords_ls]\n",
    "#         #join words back together\n",
    "#         clean = ' '.join(filtered_words)\n",
    "#         #create porter stemmer\n",
    "#         ps = nltk.porter.PorterStemmer()\n",
    "#         stems = [ps.stem(word) for word in clean.split()]\n",
    "#         #join words back together\n",
    "#         stemmed = ' '.join(stems)\n",
    "#         #create the lemmatizer\n",
    "#         wnl = nltk.stem.WordNetLemmatizer()\n",
    "#         lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "#         #join words back together\n",
    "#         lemmatized = ' '.join(lemmas)\n",
    "#     original.rename(columns = {'content':'original'}, inplace = True)\n",
    "#     original['clean'] = clean\n",
    "#     original['stemmed'] = stemmed\n",
    "#     original['lemmatized'] = lemmatized\n",
    "#     return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27506218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_article(original):\n",
    "    original['clean'] = original['content'].apply(basic_clean).apply(tokenize).apply(remove_stopwords)\n",
    "    original['stemmed'] = original['content'].apply(basic_clean).apply(tokenize).apply(remove_stopwords).apply(stem)\n",
    "    original['lemmatized'] = original['content'].apply(basic_clean).apply(tokenize).apply(remove_stopwords).apply(lemmatize)\n",
    "    original.rename(columns = {'content':'original'}, inplace = True)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd26c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = original.content[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae5f550",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /Users/mph/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /Users/mph/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d24f05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excited to launch our first diversity equity, and inclusion (dei) report! in over eight years as an organization, weve implemented policies and grown our dei efforts. we are extremely proud of the progress weve made as a staff and codeup community, and we recognize there is more to learn. this report captures some of the ways that weve lived our value of cultivating inclusive growth, and how we will continue doing so as we look to the future.\\nwe wanted to shine a light on the demographics of our students and staff, and in particular how that compares to the tech industry as a whole. how we collect, organize, and share employee demographic data is informed by standards set by the equal employment opportunity commission (eeoc).\\nwe are proud to celebrate how weve grown and are motivated and committed to do more and be better. to view the report visit the link here, or download it below.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    ".encode('ascii', 'ignore')\\\n",
    ".decode('utf-8')\n",
    "\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a1d341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excited to launch our first diversity equity and inclusion dei report in over eight years as an organization weve implemented policies and grown our dei efforts we are extremely proud of the progress weve made as a staff and codeup community and we recognize there is more to learn this report captures some of the ways that weve lived our value of cultivating inclusive growth and how we will continue doing so as we look to the future\\nwe wanted to shine a light on the demographics of our students and staff and in particular how that compares to the tech industry as a whole how we collect organize and share employee demographic data is informed by standards set by the equal employment opportunity commission eeoc\\nwe are proud to celebrate how weve grown and are motivated and committed to do more and be better to view the report visit the link here or download it below'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use re.sub to remove special characters\n",
    "article = re.sub(r'[^a-z0-9\\'\\s]', '', article)\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c23a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.toktok.ToktokTokenizer at 0x159ce3ca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the tokenizer\n",
    "tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3403d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excited to launch our first diversity equity and inclusion dei report in over eight years as an organization weve implemented policies and grown our dei efforts we are extremely proud of the progress weve made as a staff and codeup community and we recognize there is more to learn this report captures some of the ways that weve lived our value of cultivating inclusive growth and how we will continue doing so as we look to the future\\nwe wanted to shine a light on the demographics of our students and staff and in particular how that compares to the tech industry as a whole how we collect organize and share employee demographic data is informed by standards set by the equal employment opportunity commission eeoc\\nwe are proud to celebrate how weve grown and are motivated and committed to do more and be better to view the report visit the link here or download it below'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the tokenizer\n",
    "article = tokenize.tokenize(article, return_str=True)\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f42f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save stopwords\n",
    "stopwords_ls = stopwords.words('english')\n",
    "stopwords_ls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a320104",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = article.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0f42a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codeup',\n",
       " 'excited',\n",
       " 'launch',\n",
       " 'first',\n",
       " 'diversity',\n",
       " 'equity',\n",
       " 'inclusion',\n",
       " 'dei',\n",
       " 'report',\n",
       " 'eight',\n",
       " 'years',\n",
       " 'organization',\n",
       " 'weve',\n",
       " 'implemented',\n",
       " 'policies',\n",
       " 'grown',\n",
       " 'dei',\n",
       " 'efforts',\n",
       " 'extremely',\n",
       " 'proud',\n",
       " 'progress',\n",
       " 'weve',\n",
       " 'made',\n",
       " 'staff',\n",
       " 'codeup',\n",
       " 'community',\n",
       " 'recognize',\n",
       " 'learn',\n",
       " 'report',\n",
       " 'captures',\n",
       " 'ways',\n",
       " 'weve',\n",
       " 'lived',\n",
       " 'value',\n",
       " 'cultivating',\n",
       " 'inclusive',\n",
       " 'growth',\n",
       " 'continue',\n",
       " 'look',\n",
       " 'future',\n",
       " 'wanted',\n",
       " 'shine',\n",
       " 'light',\n",
       " 'demographics',\n",
       " 'students',\n",
       " 'staff',\n",
       " 'particular',\n",
       " 'compares',\n",
       " 'tech',\n",
       " 'industry',\n",
       " 'whole',\n",
       " 'collect',\n",
       " 'organize',\n",
       " 'share',\n",
       " 'employee',\n",
       " 'demographic',\n",
       " 'data',\n",
       " 'informed',\n",
       " 'standards',\n",
       " 'set',\n",
       " 'equal',\n",
       " 'employment',\n",
       " 'opportunity',\n",
       " 'commission',\n",
       " 'eeoc',\n",
       " 'proud',\n",
       " 'celebrate',\n",
       " 'weve',\n",
       " 'grown',\n",
       " 'motivated',\n",
       " 'committed',\n",
       " 'better',\n",
       " 'view',\n",
       " 'report',\n",
       " 'visit',\n",
       " 'link',\n",
       " 'download']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords from list of words\n",
    "filtered_words = [word for word in words if word not in stopwords_ls]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3409fb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show how many words we removed\n",
    "len(words) - len(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c85141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup excited launch first diversity equity inclusion dei report eight years organization weve implemented policies grown dei efforts extremely proud progress weve made staff codeup community recognize learn report captures ways weve lived value cultivating inclusive growth continue look future wanted shine light demographics students staff particular compares tech industry whole collect organize share employee demographic data informed standards set equal employment opportunity commission eeoc proud celebrate weve grown motivated committed better view report visit link download'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "107d3867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create porter stemmer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8f8663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call', 'call')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test stemmer\n",
    "ps.stem('calling'), ps.stem('calls'), ps.stem('called'), ps.stem('call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3fa6357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excited to launch our first diversity equity and inclusion dei report in over eight years as an organization weve implemented policies and grown our dei efforts we are extremely proud of the progress weve made as a staff and codeup community and we recognize there is more to learn this report captures some of the ways that weve lived our value of cultivating inclusive growth and how we will continue doing so as we look to the future\\nwe wanted to shine a light on the demographics of our students and staff and in particular how that compares to the tech industry as a whole how we collect organize and share employee demographic data is informed by standards set by the equal employment opportunity commission eeoc\\nwe are proud to celebrate how weve grown and are motivated and committed to do more and be better to view the report visit the link here or download it below'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use stemmer - apply stem to each word in our string\n",
    "ps.stem(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83b4f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [ps.stem(word) for word in article.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6e931e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excit to launch our first divers equiti and inclus dei report in over eight year as an organ weve implement polici and grown our dei effort we are extrem proud of the progress weve made as a staff and codeup commun and we recogn there is more to learn thi report captur some of the way that weve live our valu of cultiv inclus growth and how we will continu do so as we look to the futur we want to shine a light on the demograph of our student and staff and in particular how that compar to the tech industri as a whole how we collect organ and share employe demograph data is inform by standard set by the equal employ opportun commiss eeoc we are proud to celebr how weve grown and are motiv and commit to do more and be better to view the report visit the link here or download it below'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "article_stemmed = ' '.join(stems)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cff695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c958278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codeup',\n",
       " 'is',\n",
       " 'excited',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'our',\n",
       " 'first',\n",
       " 'diversity',\n",
       " 'equity',\n",
       " 'and']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use lemmatizer\n",
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c425c9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codeup is excited to launch our first diversity equity and inclusion dei report in over eight year a an organization weve implemented policy and grown our dei effort we are extremely proud of the progress weve made a a staff and codeup community and we recognize there is more to learn this report capture some of the way that weve lived our value of cultivating inclusive growth and how we will continue doing so a we look to the future we wanted to shine a light on the demographic of our student and staff and in particular how that compare to the tech industry a a whole how we collect organize and share employee demographic data is informed by standard set by the equal employment opportunity commission eeoc we are proud to celebrate how weve grown and are motivated and committed to do more and be better to view the report visit the link here or download it below'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "article_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370b796",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57d7beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = prepare_article(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "757ab957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diversity Equity and Inclusion Report</td>\n",
       "      <td>Codeup is excited to launch our first Diversit...</td>\n",
       "      <td>codeup excited launch first diversity equity i...</td>\n",
       "      <td>codeup excit launch first divers equiti inclus...</td>\n",
       "      <td>codeup excited launch first diversity equity i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup Honored as SABJ Diversity and Inclusion...</td>\n",
       "      <td>Codeup has been named the 2022 Diversity and I...</td>\n",
       "      <td>codeup named 2022 diversity inclusion award wi...</td>\n",
       "      <td>codeup name 2022 divers inclus award winner sa...</td>\n",
       "      <td>codeup named 2022 diversity inclusion award wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Can I Finance My Career Transition?</td>\n",
       "      <td>Deciding to transition into a tech career is a...</td>\n",
       "      <td>deciding transition tech career big step signi...</td>\n",
       "      <td>decid transit tech career big step signific co...</td>\n",
       "      <td>deciding transition tech career big step signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tips for Women Beginning a Career in Tech</td>\n",
       "      <td>Codeup strongly values diversity, and inclusio...</td>\n",
       "      <td>codeup strongly values diversity inclusion hon...</td>\n",
       "      <td>codeup strongli valu divers inclus honor ameri...</td>\n",
       "      <td>codeup strongly value diversity inclusion hono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Cloud Computing and AWS?</td>\n",
       "      <td>With many companies switching to cloud service...</td>\n",
       "      <td>many companies switching cloud services implem...</td>\n",
       "      <td>mani compani switch cloud servic implement clo...</td>\n",
       "      <td>many company switching cloud service implement...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              Diversity Equity and Inclusion Report   \n",
       "1  Codeup Honored as SABJ Diversity and Inclusion...   \n",
       "2            How Can I Finance My Career Transition?   \n",
       "3          Tips for Women Beginning a Career in Tech   \n",
       "4                   What is Cloud Computing and AWS?   \n",
       "\n",
       "                                            original  \\\n",
       "0  Codeup is excited to launch our first Diversit...   \n",
       "1  Codeup has been named the 2022 Diversity and I...   \n",
       "2  Deciding to transition into a tech career is a...   \n",
       "3  Codeup strongly values diversity, and inclusio...   \n",
       "4  With many companies switching to cloud service...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  codeup excited launch first diversity equity i...   \n",
       "1  codeup named 2022 diversity inclusion award wi...   \n",
       "2  deciding transition tech career big step signi...   \n",
       "3  codeup strongly values diversity inclusion hon...   \n",
       "4  many companies switching cloud services implem...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  codeup excit launch first divers equiti inclus...   \n",
       "1  codeup name 2022 divers inclus award winner sa...   \n",
       "2  decid transit tech career big step signific co...   \n",
       "3  codeup strongli valu divers inclus honor ameri...   \n",
       "4  mani compani switch cloud servic implement clo...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  codeup excited launch first diversity equity i...  \n",
       "1  codeup named 2022 diversity inclusion award wi...  \n",
       "2  deciding transition tech career big step signi...  \n",
       "3  codeup strongly value diversity inclusion hono...  \n",
       "4  many company switching cloud service implement...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9805d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://inshorts.com/en/read'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bfe626a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = acquire.get_news_articles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "922896f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indian Navy gets VLF, easy communication with ...</td>\n",
       "      <td>The Indian navy has a new communication system...</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India beat NZ 3-2 to enter CWG hockey finals</td>\n",
       "      <td>In the CWG men's hockey semi-final against New...</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bharti Airtel rakes in 61% profit</td>\n",
       "      <td>Bharti Airtel, India's top telecommunications ...</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kashmir's famous Dal Lake freezes</td>\n",
       "      <td>After the recent snowfall in upper reaches of ...</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nigerian weightlifter in dope net, India may gain</td>\n",
       "      <td>India may move up after Nigerian weightlifter ...</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Withdraw rule that makes 6 airbags mandatory i...</td>\n",
       "      <td>International Road Federation (IRF) has urged ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Fix for wheel issue that caused electric car r...</td>\n",
       "      <td>Toyota Motor said it has found a fix for the d...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Mercedes-Benz sees 28% rise in sales in India ...</td>\n",
       "      <td>Mercedes-Benz India has registered 28% rise in...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>TVS Motor beats Hero MotoCorp to become 6th mo...</td>\n",
       "      <td>TVS Motor Company Limited has become the sixth...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Honda Motor, LG to build $4.4 billion EV batte...</td>\n",
       "      <td>Honda Motor and LG Energy Solution on Tuesday ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Indian Navy gets VLF, easy communication with ...   \n",
       "1         India beat NZ 3-2 to enter CWG hockey finals   \n",
       "2                    Bharti Airtel rakes in 61% profit   \n",
       "3                    Kashmir's famous Dal Lake freezes   \n",
       "4    Nigerian weightlifter in dope net, India may gain   \n",
       "..                                                 ...   \n",
       "280  Withdraw rule that makes 6 airbags mandatory i...   \n",
       "281  Fix for wheel issue that caused electric car r...   \n",
       "282  Mercedes-Benz sees 28% rise in sales in India ...   \n",
       "283  TVS Motor beats Hero MotoCorp to become 6th mo...   \n",
       "284  Honda Motor, LG to build $4.4 billion EV batte...   \n",
       "\n",
       "                                               content    category  \n",
       "0    The Indian navy has a new communication system...       india  \n",
       "1    In the CWG men's hockey semi-final against New...       india  \n",
       "2    Bharti Airtel, India's top telecommunications ...       india  \n",
       "3    After the recent snowfall in upper reaches of ...       india  \n",
       "4    India may move up after Nigerian weightlifter ...       india  \n",
       "..                                                 ...         ...  \n",
       "280  International Road Federation (IRF) has urged ...  automobile  \n",
       "281  Toyota Motor said it has found a fix for the d...  automobile  \n",
       "282  Mercedes-Benz India has registered 28% rise in...  automobile  \n",
       "283  TVS Motor Company Limited has become the sixth...  automobile  \n",
       "284  Honda Motor and LG Energy Solution on Tuesday ...  automobile  \n",
       "\n",
       "[285 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b42331c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>category</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indian Navy gets VLF, easy communication with ...</td>\n",
       "      <td>The Indian navy has a new communication system...</td>\n",
       "      <td>india</td>\n",
       "      <td>indian navy new communication system critical ...</td>\n",
       "      <td>indian navi new commun system critic pass code...</td>\n",
       "      <td>indian navy new communication system critical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India beat NZ 3-2 to enter CWG hockey finals</td>\n",
       "      <td>In the CWG men's hockey semi-final against New...</td>\n",
       "      <td>india</td>\n",
       "      <td>cwg men ' hockey semifinal new zealand saturda...</td>\n",
       "      <td>cwg men ' hockey semifin new zealand saturday ...</td>\n",
       "      <td>cwg men ' hockey semifinal new zealand saturda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bharti Airtel rakes in 61% profit</td>\n",
       "      <td>Bharti Airtel, India's top telecommunications ...</td>\n",
       "      <td>india</td>\n",
       "      <td>bharti airtel india ' top telecommunications c...</td>\n",
       "      <td>bharti airtel india ' top telecommun compani r...</td>\n",
       "      <td>bharti airtel india ' top telecommunication co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kashmir's famous Dal Lake freezes</td>\n",
       "      <td>After the recent snowfall in upper reaches of ...</td>\n",
       "      <td>india</td>\n",
       "      <td>recent snowfall upper reaches kashmir himalaya...</td>\n",
       "      <td>recent snowfal upper reach kashmir himalayan p...</td>\n",
       "      <td>recent snowfall upper reach kashmir himalayan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nigerian weightlifter in dope net, India may gain</td>\n",
       "      <td>India may move up after Nigerian weightlifter ...</td>\n",
       "      <td>india</td>\n",
       "      <td>india may move nigerian weightlifter chika ama...</td>\n",
       "      <td>india may move nigerian weightlift chika amala...</td>\n",
       "      <td>india may move nigerian weightlifter chika ama...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Indian Navy gets VLF, easy communication with ...   \n",
       "1       India beat NZ 3-2 to enter CWG hockey finals   \n",
       "2                  Bharti Airtel rakes in 61% profit   \n",
       "3                  Kashmir's famous Dal Lake freezes   \n",
       "4  Nigerian weightlifter in dope net, India may gain   \n",
       "\n",
       "                                            original category  \\\n",
       "0  The Indian navy has a new communication system...    india   \n",
       "1  In the CWG men's hockey semi-final against New...    india   \n",
       "2  Bharti Airtel, India's top telecommunications ...    india   \n",
       "3  After the recent snowfall in upper reaches of ...    india   \n",
       "4  India may move up after Nigerian weightlifter ...    india   \n",
       "\n",
       "                                               clean  \\\n",
       "0  indian navy new communication system critical ...   \n",
       "1  cwg men ' hockey semifinal new zealand saturda...   \n",
       "2  bharti airtel india ' top telecommunications c...   \n",
       "3  recent snowfall upper reaches kashmir himalaya...   \n",
       "4  india may move nigerian weightlifter chika ama...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  indian navi new commun system critic pass code...   \n",
       "1  cwg men ' hockey semifin new zealand saturday ...   \n",
       "2  bharti airtel india ' top telecommun compani r...   \n",
       "3  recent snowfal upper reach kashmir himalayan p...   \n",
       "4  india may move nigerian weightlift chika amala...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  indian navy new communication system critical ...  \n",
       "1  cwg men ' hockey semifinal new zealand saturda...  \n",
       "2  bharti airtel india ' top telecommunication co...  \n",
       "3  recent snowfall upper reach kashmir himalayan ...  \n",
       "4  india may move nigerian weightlifter chika ama...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = prepare_article(original)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176932e",
   "metadata": {},
   "source": [
    "9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97e4c7",
   "metadata": {},
   "source": [
    "Yes for 493KB and 25MB, however 200TB of data is never going to load and would result in a large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
